from typing import Optional, Dict, List, Union, Iterable

import numpy as np
import parameterspace as ps
from blackboxopt import (
    Evaluation,
    EvaluationSpecification,
)

from .meta_blr import MetaBLR


class RandomSearchSampler:
    def __init__(self, search_space: ps.ParameterSpace, seed: Optional[int] = None):
        """Random search for configurations"""
        self.search_space = search_space.copy()
        self.seed = seed
        self.search_space.seed(seed)

    def __call__(self) -> EvaluationSpecification:
        return EvaluationSpecification(configuration=self.search_space.sample())


class BaNNER:
    """Meta-learning for Bayesian optimization
    Args:
        search_space: Search space of the problem
        seed: Random seed
        num_samples_acquisition_function: Number of random samples for optimizing the
            acquisition function.
    """
    def __init__(
        self,
        search_space: ps.ParameterSpace,
        seed: Optional[int] = None,
        num_samples_acquisition_function: int = 5120,
        **blr_kwargs,
    ):

        self.search_space = search_space
        self.seed = seed
        self.rng = np.random.default_rng(seed)
        self.num_samples_acquisition_function = num_samples_acquisition_function
        self.random_sampler = RandomSearchSampler(search_space, seed=seed)

        # meta-learning blr
        self.meta_blr = MetaBLR(
            input_dim=len(search_space),
            output_dim=1,
            **blr_kwargs
        )

        # saved previous observations
        self.X = []
        self.y = []

    def meta_fit(
        self,
        meta_data: Optional[Dict[str, List[Evaluation]]] = None,
        num_epochs: int = 1024,
        batch_size: int = 64,
        **train_config
    ):
        """Meta-learning on meta-data, corresponds to the meta-learning part in Algorithm 1."""
        converted_meta_data = dict()
        for task_uid, evaluations in meta_data.items():
            X = np.array([self.search_space.to_numerical(e.configuration) for e in evaluations])
            Y = np.array([e.objectives["loss"] for e in evaluations]).reshape(-1, self.meta_blr.output_dim)
            converted_meta_data[task_uid] = {"X": X, "Y": Y}

        self.meta_blr.meta_fit(
            converted_meta_data,
            num_epochs=num_epochs,
            batch_size=batch_size,
            **train_config
        )

    def _update_model(self):
        """
        Train optimizer on new observations from the target task.
        """
        self.meta_blr.fit(self.X, self.y)

    def generate_evaluation_specification(self, **kwargs):
        """
        Propose new configuration x to evaluate.

        1. First proposal is proposed according to the maximum of the meta-learned
        acquisition function.
        2. After the first evaluation, the proposed candidate is selected based on
        the Thompson sample of the acquisition function. 
        """

        self._update_model()
        configuration = self._sample_new_config(**kwargs)
        eval_spec = EvaluationSpecification(configuration=configuration)

        return eval_spec

    def predict(self, X, **kwargs):
        """Predict the utility of given input X (acquisition function value for given points)"""
        # In order to visualize and reproduce the Thompson sampling result later,
        # we use a random seed to generate samples and save the random seed.

        mean, var = self.meta_blr.predict(X, **kwargs)
        return mean, var

    def _sample_new_config(
        self,
        seed: Optional[int] = None,
        **kwargs
    ):
        """
        Optimizing the acquisition function.

        As pointed out by the BORE paper, random search for optimizing the
        acquisition function generated by tree-based methods is better than
        using evolutionary algorithm. Therefore we use random search for
        optimizing the acquisition function. 
        """
        rng = np.random.default_rng(seed)
        samples = rng.random([
            self.num_samples_acquisition_function, len(self.search_space)
        ])
        af_samples = self.predict(samples)
        best_vector = samples[np.argmax(af_samples)]

        return self.search_space.from_numerical(best_vector)

    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -> None:
        """Report observations to the optimizer"""
        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations

        for evaluation in _evals:
            self._report(evaluation)

    def _report(self, evaluation: Evaluation) -> None:
        new_X = np.atleast_2d(self.search_space.to_numerical(evaluation.configuration))
        new_y = evaluation.objectives["loss"]

        if len(self.X) > 0:
            self.X = np.vstack([self.X, new_X])
            self.y = np.concatenate([self.y, [new_y]])
        else:
            self.X = new_X
            self.y = np.array([new_y])
